{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(Y):\n",
    "    Y = Y.flatten()\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    return one_hot_Y\n",
    "\n",
    "def softmax(Z):\n",
    "    exp = Z  # np.exp(Z)\n",
    "    A = exp / np.sum(exp)\n",
    "    # print(np.sum(exp, axis=axis))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"mnist_train.csv\", header=None).to_numpy()\n",
    "x, y = data[:, 1:]/255, one_hot_encode(data[:, 0:1])  # x.shape, y.shape = (60000, 784) (60000, output_neurons)\n",
    "# plt.imshow(x[0].reshape(28, 28), cmap=\"gray\")\n",
    "# test  = pd.read_csv(\"mnist_test.csv\", header=None).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden = 8\n",
    "# w0 = np.random.rand(hidden, 784)\n",
    "# b0 = np.random.rand(hidden)\n",
    "# w1 = np.random.rand(hidden, hidden)\n",
    "# b1 = np.random.rand(hidden)\n",
    "# w2 = np.random.rand(10, hidden)\n",
    "# b2 = np.random.rand(10)\n",
    "# def relu(x):\n",
    "# \treturn np.maximum(x, 0)\n",
    "# def relu_(x):\n",
    "# \treturn (x > 0) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a1 = relu(x @ w0.T + b0.T)\n",
    "# a2 = relu(a1 @ w1.T + b1.T)\n",
    "# a3 = softmax(a2 @ w2.T + b2.T, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input, output, alpha = 0.01) -> None:\n",
    "        self.input = input\n",
    "        self.output = output\n",
    "        self.w = np.random.rand(output, input)  # w.shape = (neurons_in_this_layer, neurons_in_prev_layer)\n",
    "        self.b = np.random.rand(output)  # b.shape = (neurons_in_this_layer,)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(x, 0)\n",
    "    def relu_(self, x):\n",
    "        return (x > 0) * 1\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        A = np.exp(Z) / sum(np.exp(Z))\n",
    "        return A\n",
    "    def softmax_(self, Z):\n",
    "        return Z\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x  # x.shape = (60000, neurons_in_prev_layer)\n",
    "        self.z = x @ self.w.T + self.b    # z.shape = (60000, neurons_in_this_layer)\n",
    "        self.a = self.activation(self.z)  # a.shape = (60000, neurons_in_this_layer)\n",
    "        return self.a\n",
    "\n",
    "    def update(self):\n",
    "        # print(self.dw.shape, self.w.shape, self.db.shape, self.b.shape, f\"{self.alpha = }\", (self.alpha * self.dw).shape)\n",
    "        # print(self.w.shape, self.dw.shape)\n",
    "        self.w = self.w - self.alpha * self.dw\n",
    "        self.b = self.b - self.alpha * self.db.mean(axis=0).T\n",
    "\n",
    "class HiddenLayer(Layer):  # let this have 16 neurons\n",
    "    def __init__(self, input, output, act_f=\"relu\") -> None:\n",
    "        super().__init__(input, output)\n",
    "        self.activation = self.relu if act_f == \"relu\" else self.softmax\n",
    "        self.activation_ = self.relu_ if act_f == \"relu\" else self.softmax_\n",
    "    \n",
    "    def backward(self, next_layer):\n",
    "        dcdb_Lp1 = next_layer.db         # dcdb_Lp1.shape = (60000, neurons_in_next_layer)\n",
    "        w_Lp1 = next_layer.w             # w_Lp1.shape    = (neurons_in_next_layer, neurons_in_this_layer)\n",
    "        prev = dcdb_Lp1 @ w_Lp1      # equivalent to dcda = (60000, neurons_in_this_layer)\n",
    "        dadz = self.activation_(self.z)  # dadz.shape     = (60000, neurons_in_this_layer)\n",
    "        dzdw = self.x                    # dzdw.shape     = (60000, neurons_in_prev_layer)\n",
    "\n",
    "        self.dw = (prev * dadz).T @ dzdw # dw.shape       = (output_neurons, neurons_in_prev_layer)\n",
    "        self.db = (prev * dadz)            # db.shape       = (60000, output_neurons)\n",
    "\n",
    "        return self\n",
    "\n",
    "class OutputLayer(Layer):\n",
    "    def __init__(self, input, output, act_f=\"softmax\") -> None:\n",
    "        super().__init__(input, output)\n",
    "        self.activation = self.softmax if act_f == \"softmax\" else self.relu\n",
    "        self.activation_ = self.softmax_ if act_f == \"softmax\" else self.relu_\n",
    "\n",
    "    def backward(self, y):               # y.shape    = (60000, output_neurons)  Y is one hot encoded\n",
    "        # C (error) = (self.a - y)^2\n",
    "        dcda = self.a - y                # dcda.shape = (60000, output_neurons)\n",
    "        dadz = self.activation_(self.z)  # dadz.shape = (60000, output_neurons)\n",
    "        dzdw = self.x                    # dzdw.shape = (60000, neurons_in_prev_layer)\n",
    "\n",
    "        self.dw = (dcda * dadz).T @ dzdw # dw.shape   = (output_neurons, neurons_in_prev_layer)\n",
    "        self.db = (dcda * dadz)          # db.shape   = (60000, output_neurons)\n",
    "\n",
    "        return self  # coz this is needed to calculate the error in the previous layer\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, alpha = 0.01, name = \"Neural Network\"):\n",
    "        self.layers = layers\n",
    "        self.name = name\n",
    "        for layer in self.layers:\n",
    "            layer.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, y):\n",
    "        for layer in reversed(self.layers):\n",
    "            y = layer.backward(y)\n",
    "        return y\n",
    "\n",
    "    def update(self):\n",
    "        for layer in reversed(self.layers):\n",
    "            layer.update()\n",
    "    \n",
    "    def train(self, x, y, epochs=10):\n",
    "        losses = []\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            y_pred = self.forward(x)\n",
    "            print(y_pred[0], y[0])\n",
    "            self.backward(y)\n",
    "            self.update()\n",
    "            \n",
    "            loss = np.sum((y_pred - y)**2)\n",
    "            losses.append(loss)\n",
    "        return losses\n",
    "            \n",
    "    def __str__(self):\n",
    "        return f\"Neural Network: {self.name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 4\n",
    "layers = [\n",
    "            HiddenLayer(784, hidden),\n",
    "            HiddenLayer(hidden, hidden),\n",
    "            OutputLayer(hidden, 10),\n",
    "        ]\n",
    "\n",
    "nn = NeuralNetwork(layers, alpha=1e-10, name=\"trial_nn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = nn.forward(x)\n",
    "# y = layers[4].backward(y_pred)\n",
    "# y = layers[3].backward(y)\n",
    "# y = layers[2].backward(y)\n",
    "# y = layers[1].backward(y)\n",
    "# y = layers[0].backward(y)\n",
    "\n",
    "\n",
    "# layers[4].update()\n",
    "# layers[3].update()\n",
    "# layers[2].update()\n",
    "# layers[1].update()\n",
    "# layers[0].update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]C:\\Users\\amukh\\AppData\\Local\\Temp\\ipykernel_11496\\3401165975.py:15: RuntimeWarning: overflow encountered in exp\n",
      "  A = np.exp(Z) / sum(np.exp(Z))\n",
      "C:\\Users\\amukh\\AppData\\Local\\Temp\\ipykernel_11496\\3401165975.py:15: RuntimeWarning: invalid value encountered in divide\n",
      "  A = np.exp(Z) / sum(np.exp(Z))\n",
      " 10%|█         | 1/10 [00:00<00:03,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000000e+000 0.00000000e+000 0.00000000e+000 3.69520835e-185\n",
      " 2.42273494e-202 5.84752008e-116 0.00000000e+000 0.00000000e+000\n",
      " 0.00000000e+000 1.12353057e-154] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:00<00:02,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan nan nan nan nan nan nan nan] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:00<00:02,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan nan nan nan nan nan nan nan] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:01<00:01,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan nan nan nan nan nan nan nan] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:01<00:01,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan nan nan nan nan nan nan nan] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:01<00:01,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan nan nan nan nan nan nan nan] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:02<00:00,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan nan nan nan nan nan nan nan] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:02<00:00,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan nan nan nan nan nan nan nan] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:02<00:00,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan nan nan nan nan nan nan nan] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan nan nan nan nan nan nan nan] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "J = nn.train(x, y, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95b59b4b0e72d3e94105c3ab4f1a1e6e746e4c2a7c235241251baf92fb36381f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
