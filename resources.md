# Resources

| Resource | Comments |
| --- | --- |
| [Ray-Tune](https://docs.ray.io/en/latest/tune/index.html) For Hyperparameter Tuning | At this stage just grid search; even random search would have worked better and faster. |
| [Activation Functions](https://youtu.be/Y9qdKsOHRjA) | Note that sigmoid activation function has nothing to do with logistic regression. |
| [Gradiet Vanishing Problem](https://youtu.be/JIWXbzRXk1I) | Gradient vanishing problem is a problem with deep neural networks. |
| [Gradient Exploding Problem](https://youtu.be/IJ9atfxFjOQ) | Gradient exploding problem is a problem with deep neural networks. |
| [ReLU and Leaky ReLU](https://youtu.be/DDBk3ZFNtJc) | `ReLU(x) = max(0, x)` is sometimes used as an activation function |
| [Softmax](https://youtu.be/8ah-qhvaQqU) | `Softmax(x) = exp(x) / sum(exp(x))` activation is used for multi-class classification. Generally only used in the oputput layer. |
| [Batch Normalization](https://youtu.be/1XMjfhEFbFA) | Batch normalization is used to normalize the input to a layer. |
