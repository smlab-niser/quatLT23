{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import quaternion\n",
    "\n",
    "class QuaternionLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    A linear layer that works with quaternion inputs and outputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(QuaternionLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight_r = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.weight_i = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.weight_j = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.weight_k = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert the input quaternion to a quaternion tensor\n",
    "        x_q = quaternion.from_float_array(x.cpu().detach().numpy())\n",
    "        x_q = torch.tensor(x_q.components(), device=x.device)\n",
    "\n",
    "        # Apply the quaternion linear transformation\n",
    "        y_r = x_q[:,0] @ self.weight_r.t()\n",
    "        y_i = x_q[:,1] @ self.weight_i.t()\n",
    "        y_j = x_q[:,2] @ self.weight_j.t()\n",
    "        y_k = x_q[:,3] @ self.weight_k.t()\n",
    "        y = y_r + y_i + y_j + y_k + self.bias\n",
    "\n",
    "        # Convert the output quaternion tensor to a quaternion\n",
    "        y_q = quaternion.as_quat_array(torch.stack([y_r, y_i, y_j, y_k], dim=-1).cpu().detach().numpy())\n",
    "        y_q = torch.tensor(y_q.components(), device=x.device)\n",
    "\n",
    "        return y_q\n",
    "\n",
    "\n",
    "class LeNet300_100(nn.Module):\n",
    "    \"\"\"\n",
    "    A modified version of LeNet-300-100 that works with quaternion inputs and outputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes):\n",
    "        super(LeNet300_100, self).__init__()\n",
    "        self.fc1 = QuaternionLinear(784, 300)\n",
    "        self.fc2 = QuaternionLinear(300, 100)\n",
    "        self.fc3 = QuaternionLinear(100, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input tensor\n",
    "        x = nn.functional.sigmoid(self.fc1(x))\n",
    "        x = nn.functional.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Create the model and optimizer\n",
    "model = LeNet300_100(num_classes=10)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import quaternion\n",
    "\n",
    "class LeNet300_100(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet300_100, self).__init__()\n",
    "        \n",
    "        # Fully connected layers with quaternion activation functions\n",
    "        self.fc1 = nn.Linear(784, 300)\n",
    "        self.qact1 = nn.QuaternionLinear(300, 300)\n",
    "        self.fc2 = nn.Linear(300, 100)\n",
    "        self.qact2 = nn.QuaternionLinear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "        \n",
    "        # Initialize the weight and bias parameters of the fully connected layers\n",
    "        nn.init.normal_(self.fc1.weight, mean=0.0, std=0.1)\n",
    "        nn.init.normal_(self.fc2.weight, mean=0.0, std=0.1)\n",
    "        nn.init.normal_(self.fc3.weight, mean=0.0, std=0.1)\n",
    "        nn.init.constant_(self.fc1.bias, 0.1)\n",
    "        nn.init.constant_(self.fc2.bias, 0.1)\n",
    "        nn.init.constant_(self.fc3.bias, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input image\n",
    "        x = x.view(-1, 784)\n",
    "        \n",
    "        # Apply fully connected layers with quaternion activation functions\n",
    "        x = self.fc1(x)\n",
    "        x = self.qact1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.qact2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        # Return the output logits\n",
    "        return x\n",
    "    \n",
    "# Create an instance of the LeNet300_100 model\n",
    "model = LeNet300_100()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Train the model on the MNIST dataset\n",
    "for epoch in range(10):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print the loss every 100 batches\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/10], Batch [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use the nn.QuaternionLinear module provided by PyTorch to implement fully connected layers with quaternion activation functions. We also initialize the weight and bias parameters of the fully connected layers using the nn.init module. Finally, we define the loss function and optimizer and train the model on the MNIST dataset using a nested loop over the epochs and batches. Note that you will need to preprocess the MNIST dataset to convert the images and labels to PyTorch tensors and load them using a DataLoader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import quaternion\n",
    "\n",
    "class LeNetQuat(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNetQuat, self).__init__()\n",
    "        # Define the layers for the real-valued input\n",
    "        self.fc1_real = nn.Linear(784, 300)\n",
    "        self.fc2_real = nn.Linear(300, 100)\n",
    "        self.fc3_real = nn.Linear(100, 10)\n",
    "        self.relu_real = nn.ReLU()\n",
    "        \n",
    "        # Define the layers for the quaternion-valued input\n",
    "        self.fc1_quat = nn.Linear(4, 300)\n",
    "        self.fc2_quat = nn.Linear(300, 100)\n",
    "        self.fc3_quat = nn.Linear(100, 10)\n",
    "        self.relu_quat = nn.ReLU()\n",
    "        \n",
    "        # Define the output layer for the quaternion-valued input\n",
    "        self.out_quat = nn.Linear(10, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Separate the real and imaginary parts of the input\n",
    "        x_real = x[:, :784]\n",
    "        x_quat = x[:, 784:]\n",
    "        \n",
    "        # Compute the output for the real-valued input\n",
    "        x_real = self.fc1_real(x_real)\n",
    "        x_real = self.relu_real(x_real)\n",
    "        x_real = self.fc2_real(x_real)\n",
    "        x_real = self.relu_real(x_real)\n",
    "        x_real = self.fc3_real(x_real)\n",
    "        \n",
    "        # Compute the output for the quaternion-valued input\n",
    "        x_quat = quaternion.from_float_array(x_quat.detach().numpy()).normalized()\n",
    "        x_quat = quaternion.as_float_array(x_quat * quaternion.from_float_array([1, 0, 0, 0]))\n",
    "        x_quat = torch.tensor(x_quat)\n",
    "        x_quat = self.fc1_quat(x_quat)\n",
    "        x_quat = self.relu_quat(x_quat)\n",
    "        x_quat = self.fc2_quat(x_quat)\n",
    "        x_quat = self.relu_quat(x_quat)\n",
    "        x_quat = self.fc3_quat(x_quat)\n",
    "        x_quat = self.out_quat(x_quat)\n",
    "        x_quat = x_quat * x_quat\n",
    "        x_quat = x_quat.sum(dim=1)\n",
    "        x_quat = torch.sqrt(x_quat)\n",
    "        \n",
    "        # Combine the output for the real and quaternion-valued inputs\n",
    "        x = torch.cat([x_real, x_quat], dim=1)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we define a modified version of the LeNet-300-100 architecture that can take both real and quaternion-valued inputs. The input is split into two parts: the first 784 elements correspond to the real-valued input (i.e., an image of a handwritten digit), and the last 4 elements correspond to the quaternion-valued input.\n",
    "\n",
    "The real-valued input is processed using the standard LeNet-300-100 architecture, while the quaternion-valued input is first converted to a quaternion using the quaternion library, then normalized and converted back to a float tensor. The quaternion tensor is then processed using a modified LeNet-300-100 architecture that is specifically designed to work with quaternion-valued inputs. The output from the quaternion branch is then combined with the output from the real-valued branch using concatenation, and the final output is returned.\n",
    "\n",
    "Note that this is just one possible way to modify the LeNet-300-100 architecture to work with quaternions, and there are likely"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
